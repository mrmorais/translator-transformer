{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d9caa8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pt</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uma menina chorando abriu a porta.</td>\n",
       "      <td>A crying girl opened the door.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vamos tentar alguma coisa!</td>\n",
       "      <td>Let's try something.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Preciso ir dormir.</td>\n",
       "      <td>I have to go to sleep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Preciso ir dormir.</td>\n",
       "      <td>I need to go to sleep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O sinal '&amp;' significa 'e'.</td>\n",
       "      <td>The sign '&amp;' stands for 'and'.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295629</th>\n",
       "      <td>Somos da França.</td>\n",
       "      <td>We're from France.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295630</th>\n",
       "      <td>Nós somos da França.</td>\n",
       "      <td>We're from France.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295631</th>\n",
       "      <td>Tom está apenas se divertindo.</td>\n",
       "      <td>Tom is just having fun.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295632</th>\n",
       "      <td>Ele faleceu? Eu nem sabia que ele estava doente!</td>\n",
       "      <td>He's dead? I didn't even know he was sick!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295633</th>\n",
       "      <td>Não existe cerveja no Havaí.</td>\n",
       "      <td>There's no beer in Hawaii.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256974 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      pt  \\\n",
       "0                     Uma menina chorando abriu a porta.   \n",
       "1                             Vamos tentar alguma coisa!   \n",
       "2                                     Preciso ir dormir.   \n",
       "3                                     Preciso ir dormir.   \n",
       "4                             O sinal '&' significa 'e'.   \n",
       "...                                                  ...   \n",
       "295629                                  Somos da França.   \n",
       "295630                              Nós somos da França.   \n",
       "295631                    Tom está apenas se divertindo.   \n",
       "295632  Ele faleceu? Eu nem sabia que ele estava doente!   \n",
       "295633                      Não existe cerveja no Havaí.   \n",
       "\n",
       "                                                en  \n",
       "0                   A crying girl opened the door.  \n",
       "1                             Let's try something.  \n",
       "2                           I have to go to sleep.  \n",
       "3                           I need to go to sleep.  \n",
       "4                   The sign '&' stands for 'and'.  \n",
       "...                                            ...  \n",
       "295629                          We're from France.  \n",
       "295630                          We're from France.  \n",
       "295631                     Tom is just having fun.  \n",
       "295632  He's dead? I didn't even know he was sick!  \n",
       "295633                  There's no beer in Hawaii.  \n",
       "\n",
       "[256974 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "translations = pd.read_csv('data/pt-en.tsv', delimiter='\\t', encoding='utf-8', header=None)\n",
    "translations = translations[[1, 3]].rename(columns={1: 'pt', 3: 'en'})\n",
    "translations = translations.dropna()\n",
    "\n",
    "translations = translations[translations['pt'].str.split().apply(len) <= 10]\n",
    "translations = translations[translations['en'].str.split().apply(len) <= 10]\n",
    "\n",
    "translations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe67fa6",
   "metadata": {},
   "source": [
    "Just following the step-by-step [tutorial from Tokenizers library](https://huggingface.co/docs/tokenizers/pipeline#all-together-a-bert-tokenizer-from-scratch) for creating a Bert tokenizer on our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37d33bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, normalizers, decoders\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "def create_tokenizer():\n",
    "    bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "    bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "    bert_tokenizer.pre_tokenizer = Whitespace() # Splitting by whitespace and punctuations\n",
    "\n",
    "    bert_tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"[CLS] $A [SEP]\",\n",
    "        pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "        special_tokens=[\n",
    "            (\"[CLS]\", 1),\n",
    "            (\"[SEP]\", 2),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    bert_tokenizer.decoder = decoders.WordPiece()\n",
    "\n",
    "    trainer = WordPieceTrainer(vocab_size=30522, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "    return bert_tokenizer, trainer\n",
    "\n",
    "pt_tokenizer, pt_trainer = create_tokenizer()\n",
    "en_tokenizer, en_trainer = create_tokenizer()\n",
    "\n",
    "pt_tokenizer.train_from_iterator(translations['pt'], pt_trainer)\n",
    "en_tokenizer.train_from_iterator(translations['en'], en_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "feb0b6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3166, 14, 313, 317, 32, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'oi, como vai?'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pt_tokenizer.encode(\"Oi, como vai?\")\n",
    "print(output.ids)\n",
    "\n",
    "pt_tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "30100767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2596, 16, 272, 207, 152, 34, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hi, how are you?'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = en_tokenizer.encode(\"Hi, how are you?\")\n",
    "print(output.ids)\n",
    "\n",
    "en_tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b2e007e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets save our tokenizers\n",
    "\n",
    "pt_tokenizer.save('data/pt-tokens.json')\n",
    "en_tokenizer.save('data/en-tokens.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
