{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d9caa8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pt</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uma menina chorando abriu a porta.</td>\n",
       "      <td>A crying girl opened the door.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vamos tentar alguma coisa!</td>\n",
       "      <td>Let's try something.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Preciso ir dormir.</td>\n",
       "      <td>I have to go to sleep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Preciso ir dormir.</td>\n",
       "      <td>I need to go to sleep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O sinal '&amp;' significa 'e'.</td>\n",
       "      <td>The sign '&amp;' stands for 'and'.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295629</th>\n",
       "      <td>Somos da França.</td>\n",
       "      <td>We're from France.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295630</th>\n",
       "      <td>Nós somos da França.</td>\n",
       "      <td>We're from France.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295631</th>\n",
       "      <td>Tom está apenas se divertindo.</td>\n",
       "      <td>Tom is just having fun.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295632</th>\n",
       "      <td>Ele faleceu? Eu nem sabia que ele estava doente!</td>\n",
       "      <td>He's dead? I didn't even know he was sick!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295633</th>\n",
       "      <td>Não existe cerveja no Havaí.</td>\n",
       "      <td>There's no beer in Hawaii.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256974 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      pt  \\\n",
       "0                     Uma menina chorando abriu a porta.   \n",
       "1                             Vamos tentar alguma coisa!   \n",
       "2                                     Preciso ir dormir.   \n",
       "3                                     Preciso ir dormir.   \n",
       "4                             O sinal '&' significa 'e'.   \n",
       "...                                                  ...   \n",
       "295629                                  Somos da França.   \n",
       "295630                              Nós somos da França.   \n",
       "295631                    Tom está apenas se divertindo.   \n",
       "295632  Ele faleceu? Eu nem sabia que ele estava doente!   \n",
       "295633                      Não existe cerveja no Havaí.   \n",
       "\n",
       "                                                en  \n",
       "0                   A crying girl opened the door.  \n",
       "1                             Let's try something.  \n",
       "2                           I have to go to sleep.  \n",
       "3                           I need to go to sleep.  \n",
       "4                   The sign '&' stands for 'and'.  \n",
       "...                                            ...  \n",
       "295629                          We're from France.  \n",
       "295630                          We're from France.  \n",
       "295631                     Tom is just having fun.  \n",
       "295632  He's dead? I didn't even know he was sick!  \n",
       "295633                  There's no beer in Hawaii.  \n",
       "\n",
       "[256974 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "translations = pd.read_csv('data/pt-en.tsv', sep='\\t', encoding='utf-8', header=None)\n",
    "translations = translations[[1, 3]].rename(columns={1: 'pt', 3: 'en'})\n",
    "translations = translations.dropna()\n",
    "\n",
    "translations = translations[translations['pt'].str.split().apply(len) <= 10]\n",
    "translations = translations[translations['en'].str.split().apply(len) <= 10]\n",
    "\n",
    "translations = translations.reindex()\n",
    "\n",
    "translations.to_csv('data/dataset.tsv', sep='\\t')\n",
    "\n",
    "translations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe67fa6",
   "metadata": {},
   "source": [
    "Just following the step-by-step [tutorial from Tokenizers library](https://huggingface.co/docs/tokenizers/pipeline#all-together-a-bert-tokenizer-from-scratch) for creating a Bert tokenizer on our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4be8ace5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.tokenizer import BertTokenizer\n",
    "\n",
    "pt_tokenizer = BertTokenizer()\n",
    "pt_tokenizer.train_from_iterator(translations['pt'])\n",
    "\n",
    "en_tokenizer = BertTokenizer()\n",
    "en_tokenizer.train_from_iterator(translations['en'])\n",
    "\n",
    "pt_tokenizer.tokenizer.save('data/pt-tokens.json')\n",
    "en_tokenizer.tokenizer.save('data/en-tokens.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfd73898",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'tokenizer' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Lets read the tokenizer from the files saved\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m pt_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/pt-tokens.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m en_tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/en-tokens.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Code/att-trad/repo/src/tokenizer.py:13\u001b[0m, in \u001b[0;36mBertTokenizer.__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_file(path)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     16\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(WordPiece(unk_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'tokenizer' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "from src.tokenizer import BertTokenizer\n",
    "# Lets read the tokenizer from the files saved\n",
    "pt_tokenizer = BertTokenizer(path='data/pt-tokens.json')\n",
    "en_tokenizer = BertTokenizer(path='data/en-tokens.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feb0b6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3166, 14, 313, 317, 32, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'oi, como vai?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pt_tokenizer.encode(\"Oi, como vai?\")\n",
    "print(output.ids)\n",
    "\n",
    "pt_tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30100767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2596, 16, 272, 207, 152, 34, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hi, how are you?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = en_tokenizer.encode(\"Hi, how are you?\")\n",
    "print(output.ids)\n",
    "\n",
    "en_tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e007e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
