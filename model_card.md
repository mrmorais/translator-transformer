## Translator Transformer

This a toy model created to validate acquired knowledge on Transformer and Attention-based architecture. It SHOULD only be used for learning purpose.

Translated content generated by the model may present bias, undesired behavior and/or not make ethical considerations.

### Dataset

Origin of the data: Portuguese to English entries from Tatoeba ([licensed under creative commons](https://tatoeba.org/pt-br/terms_of_use#section-6)), containing ~295k entries.

The training data contains short phrases translations, with fixed maximum size of 10 words at both languages.

### Training process

The obtained dataset was pre-processed by limiting the max words in sentence, and mounting a vocabulary. The model architecture consists of a simple Encoder-Decoder Transformer block. The software used for training includes, mainly: Python, Pytorch and Spacy.

### License

Both the model and datasets are open and accessible as they were trained and obtained, respectively. Articles and posts related to this content is under [CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/), and code and model weights is under general MIT License.
